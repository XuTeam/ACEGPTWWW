---
draft: false
title: "Seminar on Math of NLP"
snippet: "The μ-PRO 1.0.6 ferroelectric, ferromagnetic, and effective property modules are available now."
image:
  {
    src: "https://images.unsplash.com/photo-1594729095022-e2f6d2eece9c?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxfDB8MXxyYW5kb218MHx8Y29kZXx8fHx8fDE2Nzg4OTQ2MDc&ixlib=rb-4.0.3&q=80&w=1080",
    alt: "full stack web development",
  }
publishDate: "2019-06-30"
category: "News"
author: "Xiaoxing Cheng"
tags: [server, release]
---

### **State 1**
* Introduction to NLP, Benyou Wang (CHHK Shenzhen)  
* Word Vectors and Word Window Classification  
* Dependency Parsing  
* Recurrent Neural Networks and Language Models  
* Vanishing Gradients, Fancy RNNs, Seq2Seq  
* Machine Translation, Attention, Subword Models  
* Transformers  
* More about Transformers and Pretraining  
* Pretrained models: GPT, Llama, …  

### **State 2**
* Natural Language Generation  
* Integrating knowledge in language models  
* Bias, toxicity, and fairness  
* Retrieval Augmented Models + Knowledge  
* ConvNets, Tree Recursive Neural Networks and Constituency  
* Scaling laws for large models  
* Editing Neural Networks  

#### **Some reading materials and linking**
1. Our notes about Transformers and GPT, suggestions and comments are welcome. (add the notes in 2023MgNet here.)  
2. Stanford CS224n: Natural Language Processing with Deep Learning: https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1224/  
3. ChatGPT: https://chat.openai.com/  
4. GPT 3: https://en.wikipedia.org/wiki/GPT-3  
5. Llama 2: https://ai.meta.com/llama/  
