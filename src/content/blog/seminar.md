---
draft: false
title: "Seminar on Math of NLP"
snippet: ""
image:
  {
    src: "https://images.unsplash.com/photo-1594729095022-e2f6d2eece9c?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxfDB8MXxyYW5kb218MHx8Y29kZXx8fHx8fDE2Nzg4OTQ2MDc&ixlib=rb-4.0.3&q=80&w=1080",
    alt: "full stack web development",
  }
publishDate: "2023-06-30"
category: "News"
author: "Xu's Team"
tags: [NLP, seminar]
---

### **State 1**
1. Introduction to NLP, Benyou Wang (CHHK Shenzhen)  
2. Word Vectors and Word Window Classification  
3. Dependency Parsing  
4. Recurrent Neural Networks and Language Models  
5. Vanishing Gradients, Fancy RNNs, Seq2Seq  
6. Machine Translation, Attention, Subword Models  
7. Transformers  
8. More about Transformers and Pretraining  
9. Pretrained models: GPT, Llama, â€¦  


### **State 2**
10. Natural Language Generation  
11. Integrating knowledge in language models  
12. Bias, toxicity, and fairness  
13. Retrieval Augmented Models + Knowledge  
14. ConvNets, Tree Recursive Neural Networks and Constituency  
15. Scaling laws for large models  
16. Editing Neural Networks  


#### **Some reading materials and linking**
Our notes about Transformers and GPT, suggestions and comments are welcome. (add the notes in 2023MgNet here.)  

1. Stanford CS224n: Natural Language Processing with Deep Learning: https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1224/  
2. ChatGPT: https://chat.openai.com/  
3. GPT 3: https://en.wikipedia.org/wiki/GPT-3  
4. Llama 2: https://ai.meta.com/llama/  
